<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Integration &amp; Toolbox Components &mdash; DataCloud Documentation 1.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/sphinx_rtd_size.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=f2a433a1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Integrated Toolbox Documentation" href="b_documentation.html" />
    <link rel="prev" title="Welcome to the online documentation of DataCloud Project and DataCloud Toolbox!" href="../index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >

          
          
          <a href="../index.html">
            
              <img src="../_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Integration &amp; Toolbox Components</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pipeline-discovery">Pipeline Discovery</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dis-pipe-features">DIS-PIPE Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="#integration-with-the-toolbox-components">Integration with the Toolbox Components</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deployment-code-and-documentation-availability">Deployment, Code and Documentation Availability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pipeline-definition">Pipeline Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#def-pipe-features">DEF-PIPE Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">Integration with the Toolbox Components</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">Deployment, Code and Documentation Availability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pipeline-simulation">Pipeline Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sim-pipe-features">SIM-PIPE Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">Integration with the Toolbox Components</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">Deployment, Code and Documentation Availability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pipeline-scheduling-and-adaptation">Pipeline Scheduling and Adaptation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ada-pipe-features">ADA-PIPE Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ada-pipe-architecture">ADA-PIPE Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">Integration with the Toolbox Components</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id10">Deployment, Code and Documentation Availability</a></li>
<li class="toctree-l4"><a class="reference internal" href="#decentralized-resource-marketplace-r-market">Decentralized Resource Marketplace (R-MARKET)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#r-market-features">R-MARKET Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id11">Integration with the Toolbox Components</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id12">Deployment, Code and Documentation Availability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pipeline-deployment-management">Pipeline Deployment &amp; Management</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dep-pipe-features">DEP-PIPE Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id13">Integration with the Toolbox Components</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id14">Deployment, Code and Documentation Availability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#runtime-dashboard-and-common-datacloud-ui">Runtime Dashboard and Common DataCLoud UI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common-identity-management">Common Identity Management</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="b_documentation.html">Integrated Toolbox Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="c_usage.html">Toolbox Usage</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #343131" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DataCloud Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Integration &amp; Toolbox Components</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/UsageGuide/a_integration_and_components.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="integration-toolbox-components">
<h1>Integration &amp; Toolbox Components<a class="headerlink" href="#integration-toolbox-components" title="Link to this heading"></a></h1>
<p class="linemarker linemarker-5">Here you can find instructions for the installation of PUZZLE Platform.
This page references the repository under <a class="reference external" href="https://gitlab.com/puzzle-project/platform-setup">https://gitlab.com/puzzle-project/platform-setup</a></p>
<section id="pipeline-discovery">
<h2>Pipeline Discovery<a class="headerlink" href="#pipeline-discovery" title="Link to this heading"></a></h2>
<p class="linemarker linemarker-13">DIS-PIPE provides scalable integration of process mining techniques and Artificial Intelligence (AI) algorithms to learn pipelines’ structure by extracting, processing and interpreting vast amounts of event data collected from several heterogeneous data sources. Furthermore, DIS‑PIPE includes a graphical interface that supports various analytics techniques for visualising the discovered pipelines together with detailed diagnostics information about their execution, and a public API that enables external user interaction, including integration with DEF-PIPE.</p>
<p class="linemarker linemarker-15">DIS-PIPE imports an event log in the XES format, containing traces associated with one or more pipeline executions. DIS-PIPE uses this event log to feed the preprocessing and discovery components for learning the pipeline models underlying the behaviours observed in the log. The importLog function can be invoked by clicking a dedicated button in the Map view of the GUI</p>
<img alt="../_images/positionGUIimportLog.png" src="../_images/positionGUIimportLog.png" />
<section id="dis-pipe-features">
<h3>DIS-PIPE Features<a class="headerlink" href="#dis-pipe-features" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-22">Below is the most recent overview of DIS-PIPE’s features, which include Event Abstraction, Segmentation and Trace Preprocessing, Dark Data Discovery and Exploitation, and Interface Functions.</p>
<ul class="simple">
<li><p class="linemarker linemarker-24"><strong>Event Data Analysis:</strong> DIS-PIPE is able to analyse event data collected from various sources and related to a single (or multiple) data pipeline(s).</p></li>
<li><p class="linemarker linemarker-25"><strong>Interpretation of Partially Ordered Event Data:</strong> DIS-PIPE interprets event data that are recorded in a partially ordered way, i.e., without explicit timestamp associated with them or with a timestamp having a coarse granularity (e.g., days).​</p></li>
<li><p class="linemarker linemarker-26"><strong>Low-frequency Trace Filtering:</strong> DIS-PIPE is able to recognise traces of event data with a low execution frequency and filter out them from the subsequent analysis.</p></li>
<li><p class="linemarker linemarker-27"><strong>Rule Filter:</strong> DIS-PIPE enable to specify complex rules in the form of ordering constraint between pipeline steps.</p></li>
<li><p class="linemarker linemarker-28"><strong>Event Logs Generation and Interpretation:</strong> DIS-PIPE is able to produce and interpret event logs formatted in XES, MXML, or CSV​.</p></li>
<li><p class="linemarker linemarker-29"><strong>Big Data Pipeline Discovery:</strong> DIS-PIPE is able to discover big data pipelines starting from event logs formatted in XES, MXML, or CSV</p></li>
<li><p class="linemarker linemarker-30"><strong>Pipeline Expressiveness:</strong> DIS-PIPE is able to discover big data pipelines, including XOR, OR, AND, loops, and data association constructs.</p></li>
<li><p class="linemarker linemarker-31"><strong>Pipeline Visualisation and Description:</strong> DIS-PIPE allows users to have a visualisation and a description of the running pipelines.</p></li>
<li><p class="linemarker linemarker-32"><strong>Pipeline Analytics:</strong> DIS-PIPE computes analytics on the discovered pipelines.</p></li>
<li><p class="linemarker linemarker-33"><strong>Conformance Checking:</strong> DIS-PIPE performs conformance checking on the discovered pipelines and those developed in the past.</p></li>
<li><p class="linemarker linemarker-34"><strong>Anomaly Detection:</strong> DIS-PIPE interprets the compliance checking outcomes signalling nonconforming deviations and providing repair solutions</p></li>
<li><p class="linemarker linemarker-35"><a href="#id1"><span class="problematic" id="id2">**</span></a>Interface Functions: <a href="#id3"><span class="problematic" id="id4">**</span></a>Development of the missing interface functions to import a DSL pipeline from DEF-PIPE and to export an event log in XES format.</p></li>
<li><p class="linemarker linemarker-36"><strong>Dark Data Discovery and Exploitation:</strong> Development of a querying algorithm to extract information related to the dark data manipulated by the data pipeline.</p></li>
<li><p class="linemarker linemarker-37"><strong>Segmentation and Trace Preprocessing:</strong> Implementing and testing of the algorithm that performs segmentation and trace preprocessing.</p></li>
<li><p class="linemarker linemarker-38"><strong>Event Abstraction:</strong> Implementing and testing of the algorithm that performs event abstraction.</p></li>
</ul>
</section>
<section id="integration-with-the-toolbox-components">
<h3>Integration with the Toolbox Components<a class="headerlink" href="#integration-with-the-toolbox-components" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-43">DIS-PIPE interfaces with DEF-PIPE and data scientists using the following six functions</p>
<img alt="../_images/disPipeInterfaces.png" src="../_images/disPipeInterfaces.png" />
<ul class="simple">
<li><p class="linemarker linemarker-47"><em>importLog(xesLogID)</em> imports an event log in the XES format containing traces associated with one or more pipeline executions. DIS-PIPE uses this event log to feed the pre-processing and discovery components for learning the pipeline models underlying the behaviours observed in the log. The importLog function can be invoked by clicking a dedicated button in the Map view of the GUI.</p></li>
<li><p class="linemarker linemarker-48"><em>exportLog(xesLogID)</em> exports an input event log, previously pre-processed and interpreted by DIS-PIPE for performing model discovery in the XES format. Data scientists use this log to feed other process discovery tools available in the market, search for a different pipeline model, or perform diagnostics further than DIS-PIPE.</p></li>
<li><p class="linemarker linemarker-49"><em>importPipeline(pipelineID)</em> imports a pipeline represented in the DSL format, potentially modified by DEF-PIPE. DIS-PIPE analyses the model’s conformance compared to the actual event logs, detects misalignments between the observed behaviour stored in the log and the new expected behaviour, and suggests recovery or realignment strategies.</p></li>
<li><p class="linemarker linemarker-50"><em>exportPipeline(pipelineID)</em> exports a discovered pipeline model, converting it into DSL format (as returned by DEF-PIPE). Specifically, DEF-PIPE invokes the exportPipeline(pipelineID) interface provided by DIS-PIPE to load a discovered DSL pipeline model and enhance/customize its description using its visual workbench.</p></li>
<li><p class="linemarker linemarker-51"><em>renamePipeline(pipelineID, newName)</em> operation allows users to update the name of an existing pipeline. It’s useful for maintaining accurate records and categorizing pipelines within DIS-PIPE. The user parameter specifies the user initiating the operation, the pipelineID parameter identifies the ID of the specific pipeline to be renamed, and the newName parameter defines the desired new name for the pipeline.</p></li>
<li><p class="linemarker linemarker-52"><em>getRepository()</em> allows analyzing the DIS-PIPE repository where XES and DSL files are stored. A key-value map is returned, where the keys are the stored pipelines’ IDs, and the values are represented by their names.</p></li>
</ul>
<p class="linemarker linemarker-53">DIS-PIPE with external tools,  as Flask REST-APIs accessible via SwaggerUI, as shown below</p>
<img alt="../_images/swaggerDisPipe.png" src="../_images/swaggerDisPipe.png" />
</section>
<section id="deployment-code-and-documentation-availability">
<h3>Deployment, Code and Documentation Availability<a class="headerlink" href="#deployment-code-and-documentation-availability" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-60">Concerning DIS-PIPE code and documentation, the tool is available on GitHub at the following link: <a class="reference external" href="https://github.com/DataCloud-project/DIS-PIPE">https://github.com/DataCloud-project/DIS-PIPE</a></p>
<p class="linemarker linemarker-62">DIS-PIPE is deployed on a single host accessible from the browser at following link: <a class="reference external" href="https://195.231.61.196:7778/">https://195.231.61.196:7778/</a> (or at <a class="reference external" href="https://datacloud-dis.euprojects.net/">https://datacloud-dis.euprojects.net/</a>). The user’s browser first triggers an incoming request. Afterward, the backend of the tool, which is in charge to load the GUI and allow users to interact with its provided functionalities, manages and processes the user’s request.</p>
<img alt="../_images/disPipeDeployment.png" src="../_images/disPipeDeployment.png" />
</section>
</section>
<section id="pipeline-definition">
<h2>Pipeline Definition<a class="headerlink" href="#pipeline-definition" title="Link to this heading"></a></h2>
<p class="linemarker linemarker-69">DEF-PIPE provides a visual design for domain experts to implement Big Data pipelines based on a DSL, including means to store and load the pipeline definitions. Furthermore, it enables data scientists to define the pipelines by configuring each step, injecting code, or customising predefined generic templates.</p>
<p class="linemarker linemarker-71">The DEF-PIPE Frontend is a graphic pipeline designer tool for defining Big Data pipelines and transforming them to DSL. The main part of the application is the interface for designing big data pipelines. This interface is implemented as a single page application using ReactJS. The popularity and stability of ReactJS make it potentially more friendly with developers to continue with the project later on. The project also uses Bootstrap, a CSS-framework providing basic UI-components building blocks, which are easy to customize.</p>
<p class="linemarker linemarker-73">The backend is implemented in C# using the DotNet (.NET) framework from Microsoft. In particular, ASP.NET Core, which is the part of the NET framework for web application, is being used. It implements a web API providing a central interface for operations such as managing pipelines and templates data, transforming pipelines into DSL.</p>
<p class="linemarker linemarker-75">Database of Pipeline Designer is used to model the steps and workflow of the pipeline. The model is persisted in JSON-format, and persisted in a MongoDB database. DEF-PIPE is integrated with Keycloak as single sign-on (SSO) solution. It offers user asset management for steps and pipelines, including the ability to expose them to other users publicly. The frontend includes improved support for parametrization of steps.</p>
<p class="linemarker linemarker-77">The architecture is displayed bellow.</p>
<img alt="../_images/defPipeArchitecture.png" src="../_images/defPipeArchitecture.png" />
<ol class="arabic simple">
<li><p class="linemarker linemarker-81"><strong>Schema editor</strong> allows a graphical construction of the Big Data pipelines through drag-and-drop operations of the step descriptions stored in the template library.</p></li>
<li><p class="linemarker linemarker-82"><strong>Templates editor</strong> is a graphical component for describing pipeline steps through their parameters and resource requirements. A templates library stores the steps’ textual descriptions for subsequent pipeline design purposes.</p></li>
<li><p class="linemarker linemarker-83"><strong>DSL editor</strong> allows describing pipelines in a textual form. The templates library also stores the complete pipeline descriptions for further redesign and reuse.</p></li>
<li><p class="linemarker linemarker-84"><strong>Pipeline repository</strong> stores Big Data pipelines using their DSL representation.</p></li>
<li><p class="linemarker linemarker-85"><strong>Public API</strong> enables external interaction, including integration with the DIS-PIPE, SIM‑PIPE and ADA-PIPE tools.</p></li>
</ol>
<img alt="../_images/stepDesignerMode.jpg" src="../_images/stepDesignerMode.jpg" />
<p class="linemarker linemarker-89">Regarding pipeline representation, the tooling provides various improvements of the domain-specific language (DSL), including a grammar for specifying DSL models and implementation of editors (autocomplete + validation) in the Eclipse environment</p>
<img alt="../_images/workingDesignerFLow.jpg" src="../_images/workingDesignerFLow.jpg" />
<p class="linemarker linemarker-93">To start working with the DEF-PIPE graphical tool, use the following link: <a class="reference external" href="https://crowdserv.sys.kth.se">https://crowdserv.sys.kth.se</a></p>
<p class="linemarker linemarker-95">A quick start guide is available on GitHub: <a class="reference external" href="https://github.com/DataCloud-project/DEF-PIPE-Frontend">https://github.com/DataCloud-project/DEF-PIPE-Frontend</a></p>
<section id="def-pipe-features">
<h3>DEF-PIPE Features<a class="headerlink" href="#def-pipe-features" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-101">Below provided an overview of the features implemented and integrated with other tools:</p>
<ul class="simple">
<li><p class="linemarker linemarker-103">Provide user-based management of pipelines</p></li>
<li><p class="linemarker linemarker-104">Support private and public pipelines</p></li>
<li><p class="linemarker linemarker-105">Integration with IAM for single sign-on</p></li>
<li><p class="linemarker linemarker-106">Improvements on the UI based on the feedback</p></li>
<li><p class="linemarker linemarker-107">Integration with more stable orchestrations</p></li>
<li><p class="linemarker linemarker-108">Integration with DEP-PIPE</p></li>
<li><p class="linemarker linemarker-109">Integration with SIM-PIPE</p></li>
<li><p class="linemarker linemarker-110">Integration with DIS-PIPE</p></li>
<li><p class="linemarker linemarker-111">Integration with ADA-PIPE</p></li>
</ul>
</section>
<section id="id5">
<h3>Integration with the Toolbox Components<a class="headerlink" href="#id5" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-115">Integration with other DataCloud components is done via implementing APIs. The APIs allow a DSL description of a discovered pipeline by the DIS-PIPE tool to be presented and edited in the graphical DEF-PIPE tool. DEP-PIPE and SIM-PIPE tools are already integrated and using the provided API to retrieve, edit, and delete a pipeline.</p>
<p class="linemarker linemarker-117">The API functionality within DEF-PIPE has been further improved. While the initial implementation allowed DSL descriptions to be retrieved based on the pipeline name, we have now enhanced this feature. The updated API now utilizes the pipeline’s unique ID to fetch the DSL definition, ensuring a more precise and efficient retrieval process.</p>
<p class="linemarker linemarker-119">The adjusted export pipeline in the DSL-API documentation can be seen below. As illustrated, the API takes the user and the pipeline’s unique id to return the DSL definition of pipeline.</p>
<img alt="../_images/exportApiDefPipe.png" src="../_images/exportApiDefPipe.png" />
<p class="linemarker linemarker-123">Additionally, we announced the implementation of an API allowing users to retrieve their pipelines’ YAML file definitions. This feature complements the existing capabilities of DEF-PIPE, enabling users to access both DSL description and YAML file with ease.
DEF-PIPE APIs are described at: <a class="reference external" href="https://crowdserv.sys.kth.se/docs">https://crowdserv.sys.kth.se/docs</a></p>
<img alt="../_images/defPipedocWebPage.png" src="../_images/defPipedocWebPage.png" />
</section>
<section id="id6">
<h3>Deployment, Code and Documentation Availability<a class="headerlink" href="#id6" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-131">DEF-PIPE is available on GitHub, along with dedicated instructions for usage <a class="reference external" href="https://github.com/DataCloud-project/DEF-PIPE-Frontend">https://github.com/DataCloud-project/DEF-PIPE-Frontend</a></p>
<p class="linemarker linemarker-133">The DEF-PIPE graphical tool is also deployed at <a class="reference external" href="https://crowdserv.sys.kth.se">https://crowdserv.sys.kth.se</a></p>
</section>
</section>
<section id="pipeline-simulation">
<h2>Pipeline Simulation<a class="headerlink" href="#pipeline-simulation" title="Link to this heading"></a></h2>
<p class="linemarker linemarker-141">SIM-PIPE simulates the pipeline execution and provides final deployment configurations that conform to the hardware requirements. SIM-PIPE also provides testing functionalities, such as a sandbox for evaluating the performance of individual pipeline steps and statistical analysis of the overall pipeline performance.</p>
<p class="linemarker linemarker-143">SIM-PIPE takes a pipeline’s definition as input and outputs runtime metrics, such as CPU usage, memory usage, energy consumption, run durations, and network bandwidth. SIM-PIPE can perform dry runs of pipelines by running small and brief versions of the pipelines in a sandbox. These dry runs improve the simulations’ accuracy for deployments at larger scales and assert whether one pipeline executes successfully.</p>
<p class="linemarker linemarker-145">SIM-PIPE consists of the following components: a web graphical user interface, a controller with an API, a relational time-series database, a simulation engine, and a sandbox to execute the dry runs.</p>
<p class="linemarker linemarker-147">The key innovation implemented in SIM-PIPE leverages the container-based approach for data pipelines and is related to the following aspects:</p>
<ul class="simple">
<li><p class="linemarker linemarker-149">Novel means of leveraging container-based approach for data pipelines with sample data and different configurations to perform eventual simulations.</p></li>
<li><p class="linemarker linemarker-150">A dry run approach for generating inputs for simulators.</p></li>
</ul>
<p class="linemarker linemarker-152">The SIM-PIPE user interface is shown below. A dry run’s pipeline steps are listed on the left screen, and logs from the run are displayed with interactive resource metrics plots on the right screen.</p>
<p class="linemarker linemarker-154"><a class="reference internal" href="../_images/registration.png"><img alt="pic1" src="../_images/registration.png" style="width: 49%;" /></a> <a class="reference internal" href="../_images/save.png"><img alt="pic2" src="../_images/save.png" style="width: 49%;" /></a></p>
<section id="sim-pipe-features">
<h3>SIM-PIPE Features<a class="headerlink" href="#sim-pipe-features" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-165">SIM-PIPE provides the following high-level functionalities:</p>
<ol class="arabic simple">
<li><p class="linemarker linemarker-167">Deploying each step of a pipeline and running it in a sandbox by providing sample input.</p></li>
<li><p class="linemarker linemarker-168">Evaluating pipeline’s step performance by recording and analysing metrics about its execution to identify bottlenecks and steps to be optimised.</p></li>
<li><p class="linemarker linemarker-169">Identification of resource requirements for the pipeline by calculating step performance per resource used.</p></li>
<li><p class="linemarker linemarker-170">Proof of concept integration with KWOK1, which simulates a cluster of Kubernetes nodes, that allows testing your pipeline with different Kubernetes features, e.g., node types and capacities.</p></li>
</ol>
<p class="linemarker linemarker-172">Below a summary of the features implemented and integrated with other tools.</p>
<ul class="simple">
<li><p class="linemarker linemarker-174">GraphQL API</p></li>
<li><p class="linemarker linemarker-175">Authentication and authorisations</p></li>
<li><p class="linemarker linemarker-176">Dry run execution</p></li>
<li><p class="linemarker linemarker-177">Secure sandbox environment</p></li>
<li><p class="linemarker linemarker-178">Metrics collection</p></li>
<li><p class="linemarker linemarker-179">Integration of KWOK simulation engine</p></li>
</ul>
</section>
<section id="id7">
<h3>Integration with the Toolbox Components<a class="headerlink" href="#id7" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-185">SIM-PIPE provides a GraphQL API. GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, allows the clients to ask for exactly what they need, makes it easier to evolve APIs over time, and enables powerful developer tools.
In addition to the integration with DEF-PIPE, SIM-PIPE dry run data is used by the scheduler of ADA-PIPE. The integration of ADA-PIPE and SIM-PIPE to enhance data pipeline scheduling is shown below.</p>
<img alt="../_images/simPipeIntegrationDryRunner.png" src="../_images/simPipeIntegrationDryRunner.png" />
</section>
<section id="id8">
<h3>Deployment, Code and Documentation Availability<a class="headerlink" href="#id8" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-193">The SIM-PIPE source code is available in a single GitHub repository: <a class="reference external" href="https://github.com/DataCloud-project/SIM-PIPE">https://github.com/DataCloud-project/SIM-PIPE</a></p>
<p class="linemarker linemarker-195">It is released as open-source software under the Apache License 2.0.</p>
<p class="linemarker linemarker-197">The SIM-PIPE technical documentation, architecture description, deployment and installation guides, and user guides with examples, and contributing guide are also available in the GitHub repository. SIM-PIPE is used as a self-installed tool, that users can use. A public deployment is currently included as part of the DataCloud demo server (datacloud-toolbox.euprojects.net).</p>
<p class="linemarker linemarker-199">The API documentation is stored in the Git repository in HTML format is available at <a class="reference external" href="https://htmlpreview.github.io/?https://github.com/DataCloud-project/SIM-PIPE/blob/main/controller/public/index.html">https://htmlpreview.github.io/?https://github.com/DataCloud-project/SIM-PIPE/blob/main/controller/public/index.html</a></p>
<p class="linemarker linemarker-201">Developers experienced with GraphQL can discover the API using any GraphQL tool, such as <a class="reference external" href="https://github.com/graphql/graphiql">GraphiQL</a> or <a class="reference external" href="https://insomnia.rest">Insomnia</a>.</p>
</section>
</section>
<section id="pipeline-scheduling-and-adaptation">
<h2>Pipeline Scheduling and Adaptation<a class="headerlink" href="#pipeline-scheduling-and-adaptation" title="Link to this heading"></a></h2>
<p class="linemarker linemarker-207">ADA-PIPE provides a data-aware algorithm for optimised and adaptable scheduling on the Computing Continuum with infrastructure drift adaptation capability. ADA-PIPE allows to reconfigure the resource allocation to pipeline steps to fulfil the service level agreement between user’s and the resource providers.</p>
<section id="ada-pipe-features">
<h3>ADA-PIPE Features<a class="headerlink" href="#ada-pipe-features" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-212">Here is an overview of the features implemented and integrated with other tools:</p>
<ul class="simple">
<li><p class="linemarker linemarker-214">Scheduling tool implementation for QoS Guarantee for Tasks with Strict Deadlines and Data-Aware Pipeline Scheduling</p></li>
<li><p class="linemarker linemarker-215">Adaptation algorithm with support of Limited Dynamic Scheduling; Avoidance of Highly Utilized Resources;</p></li>
<li><p class="linemarker linemarker-216">Adaptation policies, Scheduling and scaling up/down the data pipeline executions;</p></li>
<li><p class="linemarker linemarker-217">Monitoring policies, Utilization of resources and pipeline chunks.</p></li>
</ul>
</section>
<section id="ada-pipe-architecture">
<h3>ADA-PIPE Architecture<a class="headerlink" href="#ada-pipe-architecture" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-223">The schema below illustrates the ADA-PIPE tool components related to the monitoring, followed by data preprocessing. Afterward, ADA-PIPE (re-)trains a k-means model on the pre-processed data to detect the anomalous execution of the pipeline’s steps and adapt the initial schedules.</p>
<p class="linemarker linemarker-225">In detail, to record the pipeline executions on the computing continuum, the Prometheus monitoring system imports the NetData metrics, such as processor and memory utilization, along with the network bandwidth usage and the runtime of pipeline steps. Moreover, ADA-PIPE requires to scrape the cAdvisor metric related to the pipeline steps.</p>
<p class="linemarker linemarker-227">In the second stage, ADA-PIPE preprocesses the monitoring data for the anomaly detection phase. The preprocessing phase creates a differenced, smoothed, and lagged data collection.</p>
<p class="linemarker linemarker-229">Afterward, ADA-PIPE trains an ML-based k-means model on the monitoring data. Furthermore, the model retrains on every time interval defined by the user in the presence of new data points (i.e., CPU, memory, and network usage).</p>
<p class="linemarker linemarker-231">Moreover, if a pipeline’s step requires more replicas or an event such as a device’s utilization occurs, the adaptation and scheduling component initiates reallocation.</p>
<img alt="../_images/adaPIPEarchitecture.png" src="../_images/adaPIPEarchitecture.png" />
</section>
<section id="id9">
<h3>Integration with the Toolbox Components<a class="headerlink" href="#id9" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-238">ADA-PIPE tool is running on a European Exoscale Cloud virtual machine accessible at the following IP address and domain name: <a class="reference external" href="http://194.182.187.139/">http://194.182.187.139/</a> and <a class="reference external" href="https://datacloud-ada.euprojects.net/">https://datacloud-ada.euprojects.net/</a></p>
<img alt="../_images/adapipeFunctionalities.png" src="../_images/adapipeFunctionalities.png" />
<p class="linemarker linemarker-242">Swagger UI and the API can be found in the following URL: <a class="reference external" href="http://194.182.187.13/swagger/">http://194.182.187.13/swagger/</a></p>
<img alt="../_images/adaPipeSwagger.png" src="../_images/adaPipeSwagger.png" />
<p class="linemarker linemarker-246">Here shows that ADA-PIPE is able to receive the DSL definition of the user’s pipeline defined in DEF-PIPE tool. This information includes the quantitative hardware requirements for the pipeline steps that helps ADA-PIPE to match the pipeline’s steps to the devices with sufficient resources.</p>
<img alt="../_images/adaPipeIMportingFromDef.png" src="../_images/adaPipeIMportingFromDef.png" />
<p class="linemarker linemarker-250">Following receipt of the pipeline requirements, ADA-PIPE computes schedules based on device anomalies or the resource consumption of pipeline steps, and then modifies the pipeline’s execution on computing devices. Next, using the <code class="docutils literal notranslate"><span class="pre">adaptExecution</span></code> API, it exports the schedules to DEP-PIPE for deployment (see next image). The pipeline ID, which is obtained from the DEF-PIPE toolbox, is the primary argument supplied to the <code class="docutils literal notranslate"><span class="pre">adaptExecution</span></code> API.</p>
<img alt="../_images/adaPipeExportingPipeScheduleTODep.png" src="../_images/adaPipeExportingPipeScheduleTODep.png" />
<section id="id10">
<h4>Deployment, Code and Documentation Availability<a class="headerlink" href="#id10" title="Link to this heading"></a></h4>
<p class="linemarker linemarker-258">Mainly, the source codes for the ADA-PIPE tool are available in the repository of <a class="reference external" href="https://github.com/DataCloud-project/ADA-PIPE">https://github.com/DataCloud-project/ADA-PIPE</a>. It is categorized based on the integrated C3 testbed into the DataCloud infrastructure, anomaly detection, frontend, matching-based scheduler, resource utilization and pipeline step’s replica predictions, and deployment’s update source codes available.</p>
<p class="linemarker linemarker-260">Specifically, the frontend source code is available in the repository <a class="reference external" href="https://github.com/DataCloud-project/ADA-PIPE/tree/main/frontend/">https://github.com/DataCloud-project/ADA-PIPE/tree/main/frontend/</a>.
ADA-PIPE used the Bootstrap features by following a <a class="reference external" href="https://www.youtube.com/watch?v=kMsKm53XtyA">video tutorial</a> and a <a class="reference external" href="https://www.blog.duomly.com/how-to-crate-simple-web-page-using-bootstrap-5/">document on creating a simple web page using Bootstrap</a>.
In addition, the tutorial on how to redirect the traffic from the default flask port number (i.e., 5000) to the http port number (i.e., 80) is provided. ADA-PIPE utilizes the Python flask microweb framework to provide the functionalities of the tool. This web page provides the documentation for the tool and allows the integrated DataCloud toolbox to communicate through the APIs such as <code class="docutils literal notranslate"><span class="pre">importPipeline</span></code> and <code class="docutils literal notranslate"><span class="pre">adaptExecution</span></code>.</p>
<p class="linemarker linemarker-264">ADA-PIPE receives and authorizes the user’s token, imports the domain-specific language-based model of the user’s pipeline through its ID, utilizes subprocess and JSON Python libraries, and then parses these descriptions to extract the requirements.</p>
<p class="linemarker linemarker-266">In addition, ADA-PIPE calculates the device anomalies based on the history data imported from the DEP-PIPE monitoring tool. Specifically, the anomaly detection component utilized the Netdata metrics collected through the Prometheus monitoring system accessible through the DEP-PIPE tool. Thereafter, ADA-PIPE utilizes the collected data of the running pipeline steps on the computing infrastructure to estimate and predict the number of replicas required by the pipeline steps by using a machine learning model (see details regarding the <a class="reference external" href="https://github.com/DataCloud-project/ADA-PIPE/tree/main/replica-prediction">prediction model</a>).</p>
<p class="linemarker linemarker-268">Afterward, ADA-PIPE provides a capacity-aware matching-based scheduler for data pipeline execution on the computing continuum. The base model of the matching-based scheduler requires the Python libraries: matching, networkx, operator, numpy, yaml, json.</p>
<p class="linemarker linemarker-270">ADA-PIPE currently supports to provide either the cost-optimized or time-optimized schedules. Therefore, it first analyses the requirements of each specific step. These includes processing speed, memory, storage sizes, the capabilities of the target deployment infrastructure, and the available resources in R-MARKET. Moreover, it receives the pipeline simulation from the SIM-PIPE tool, which provides the execution time along with the resource utilization of the pipeline’s execution on the computing infrastructure. Finally, ADA-PIPE relies on the Cloud virtual machine prices to estimate the user’s costs. Based on this analysis, ADA-PIPE creates an initial deployable schedule, which minimizes resource cost or time of execution of the pipeline</p>
<p class="linemarker linemarker-272">After analyzing the updates on the resource requirements of the pipeline steps, the integrated component adapts the pipeline execution. Hence, ADA-PIPE first pre-checks the updates through the horizontal pod scaling functionalities of Kubernetes. Its source code is available in the DataCloud public code repository. Thereafter, ADA-PIPE exports the required number of replicas of the executed pipeline steps through the scaling APIs of the DEP-PIPE.</p>
</section>
<section id="decentralized-resource-marketplace-r-market">
<h4>Decentralized Resource Marketplace (R-MARKET)<a class="headerlink" href="#decentralized-resource-marketplace-r-market" title="Link to this heading"></a></h4>
<p class="linemarker linemarker-278">R-MARKET deploys a decentralised hybrid permissioned and permissionless blockchain network that federates a vast set of heterogeneous resources from various providers across the Computing Continuum. R-MARKET creates a democratic marketplace of trustworthy resources and enables transparent provisioning over multiple control and network domains for external use.</p>
</section>
</section>
<section id="r-market-features">
<h3>R-MARKET Features<a class="headerlink" href="#r-market-features" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-283">Here is provide an overview of the features implemented and integrated with other tools.</p>
<ul class="simple">
<li><p class="linemarker linemarker-285">R-MARKET UI and API have been developed for hiding the Blockchain complexity</p></li>
<li><p class="linemarker linemarker-286">Addition of Edge Server to Marketplace</p></li>
<li><p class="linemarker linemarker-287">Support longer running task (Service-Task)</p></li>
<li><p class="linemarker linemarker-288">Addition of user flexibility for contract management (extension/interruption)</p></li>
<li><p class="linemarker linemarker-289">Addition of user awareness of a worker’s</p></li>
<li><p class="linemarker linemarker-290">connection status when reserved (heartbeat system)</p></li>
<li><p class="linemarker linemarker-291">Support ADA-PIPE hardware requirements in R-MARKET</p></li>
<li><p class="linemarker linemarker-292">Automatically connection of reserved worker to DEP-PIPE</p></li>
<li><p class="linemarker linemarker-293">Possibility for anyone to connect as a worker</p></li>
<li><p class="linemarker linemarker-294">Possibility for providers to specify a maximum total duration for resource usage</p></li>
<li><p class="linemarker linemarker-295">Expansion of resource stack to include various resource types (e.g., GPU accelerated VMs)</p></li>
<li><p class="linemarker linemarker-296">Implementation of smart contracts that act as proxies, automating payments for whitelisted wallets authorized to do so.</p></li>
</ul>
</section>
<section id="id11">
<h3>Integration with the Toolbox Components<a class="headerlink" href="#id11" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-302">R-MARKET is a composite tool, developed by getting the inspiration from the <a class="reference external" href="https://https://market.iex.ec/">iExec Marketplace</a>.All the related component of the R-MARKET are listed and available to the GitHub repository (<a class="reference external" href="https://github.com/DataCloud-project/R-MARKET">https://github.com/DataCloud-project/R-MARKET</a>). The main entry point of the R-MARKET tool is the R-MARKET UI (<a class="reference external" href="https://github.com/DataCloud-project/R_MARKET_UI">https://github.com/DataCloud-project/R_MARKET_UI</a>), which is a React.JS UI (Figure 28 and Figure 29) and it helps ease the interaction between other tools and R-MARKET component by hiding the complexity of Blockchain technology. This UI is deployed on the Microsoft Azure provisioned VM and available at: <a class="reference external" href="https://r-market.westeurope.cloudapp.azure.com:5555/">https://r-market.westeurope.cloudapp.azure.com:5555/</a>.</p>
<img alt="../_images/rMarketUI.png" src="../_images/rMarketUI.png" />
<img alt="../_images/rMarketUI2.png" src="../_images/rMarketUI2.png" />
<p class="linemarker linemarker-308">For authenticating the R-MARKET UI interactions, the MetaMask plugin is used and its integration is with the R-MARKET Node.JS API server and the R-MARKET back-end.</p>
<p class="linemarker linemarker-310">The R-MARKET API stands as the central communication hub within the R-MARKET ecosystem, serving as the conduit through which a wide array of functions and operations are executed. Its pivotal role lies in empowering the R-MARKET UI by granting it the capability to access, interact with, and oversee the functionalities of the decentralized marketplace. The synergy between the R-MARKET API and R-MARKET UI is mutually beneficial, with the API being responsible for executing the commands and actions initiated by users through the user interface. It operates as the responsive server that handles specific functions for the R-MARKET UI, effectively coordinating tasks such as resource provisioning, reservation, and transaction with precision and efficiency. R-MARKET API serves as a vital link, connecting users to the MetaMask decentralized marketplace. It ensures that users’ requests and commands seamlessly translate into actionable operations, ultimately fostering a marketplace experience that is both user-friendly and highly efficient.</p>
<img alt="../_images/rMarketSwagger.png" src="../_images/rMarketSwagger.png" />
<p class="linemarker linemarker-314">Besides the R-MARKET UI and API, we have already deployed the All-in-One Blockchain node over the Microsoft Azure provisioned VM, for permanently keeping the deal information and transactions into the Blockchain. In addition, we have already deployed a workerpool, along with scheduler and workers, over a Microsoft Azure provisioned VM. The details of this workerpool are available to the following link: Workerpool 1: <a class="reference external" href="http://r-market.westeurope.cloudapp.azure.com:30000/">http://r-market.westeurope.cloudapp.azure.com:30000/</a></p>
<img alt="../_images/rMarketArchitecture.png" src="../_images/rMarketArchitecture.png" />
<p class="linemarker linemarker-318">ADA-PIPE, DEP-PIPE and R-MARKET collectively constitute the Run-Time bundle. This bundle serves as the foundation for scheduling and provisioning Computing Continuum resources to facilitate the deployment of Big Data pipelines.</p>
<p class="linemarker linemarker-320">A visual depiction of these components’ integration into the run-time bundle can be seen above. This integration modification improves resource allocation, guaranteeing a more effective Big Data pipeline deployment procedure.</p>
<p class="linemarker linemarker-322">The user interfaces for the R-MARKET UI are depicted in the schema below.</p>
<img alt="../_images/rMarketINterfaces.png" src="../_images/rMarketINterfaces.png" />
</section>
<section id="id12">
<h3>Deployment, Code and Documentation Availability<a class="headerlink" href="#id12" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-329">In the main GitHub repository of the R-MARKET (<a class="reference external" href="https://github.com/DataCloud-project/R-MARKET">https://github.com/DataCloud-project/R-MARKET</a>), all the corresponding repositories for R-MARKET tool has been documented. Notably, R-MARKET has seven different repositories for the various R-MARKET components (e.g., UI, Node.JS Server, Market-API, Scheduler, Worker, etc.) followed by one repository for R-MARKET SDK. The source code and user-instruction/guidelines for each individual tool can be found in the corresponding repository.</p>
<p class="linemarker linemarker-331">For showcasing the workerpool details, the dashboard for workerpool is accessible to the following link: <a class="reference external" href="http://r-market.westeurope.cloudapp.azure.com:30000/">http://r-market.westeurope.cloudapp.azure.com:30000/</a>.</p>
<p class="linemarker linemarker-333">An overview of the current testbed, where we have implemented the R-MARKET components, is given in this schema. The Microsoft Azure platform serves as the host for this testbed. We have implemented a single workerpool in this setup, which is managed by a scheduler. We currently have our configuration maintaining a primary workerpool with at least three workers that are always connected. Furthermore, we have worker nodes that have been provided by our partners and are periodically available for task execution. We intend to extend the testbed further as part of our ongoing expansion efforts by deploying more workerpools that will integrate a wider range of resources from our partner organizations.</p>
<img alt="../_images/rMarketTestbet.png" src="../_images/rMarketTestbet.png" />
</section>
</section>
<section id="pipeline-deployment-management">
<h2>Pipeline Deployment &amp; Management<a class="headerlink" href="#pipeline-deployment-management" title="Link to this heading"></a></h2>
<p class="linemarker linemarker-341">DEP-PIPE enables flexible and scalable deployment and orchestration of Big Data pipelines over the Computing Continuum resources. DEP-PIPE monitors the pipeline execution and provides online SLO metrics to the other tools.</p>
<p class="linemarker linemarker-343">DEP-PIPE has been built to support a framework agnostic approach on top of OS-level virtualisation (containerization) techniques to allow the deployment of pipelines developed through different data analytics tools. DEP-PIPE uses resources available in cloud and edge, and also resources provided by the R-MARKET and deals with the challenges of efficient deployment and orchestration of services deployed across the computing continuum; existing state-of-the-art approaches consider the edge and the cloud devices in isolation; therefore, the main challenge is the unification of cloud, fog, and edge resources for both the deployment and orchestration of distributed applications.</p>
<p class="linemarker linemarker-345">With DEP-PIPE, the <a class="reference external" href="https://themaestro.ubitech.eu">MAESTRO</a> software platform from <a class="reference external" href="https://ubitech.eu/">UBITECH</a> and the <a class="reference external" href="https://kubernetes.io">Kubernetes</a> orchestration concepts have been extended to make them suitable for the deployment of multi-step pipelines to edge and fog environments. This leads to the development of new orchestration ideas, including workflow orchestration, for highly distributed applications.</p>
<section id="dep-pipe-features">
<h3>DEP-PIPE Features<a class="headerlink" href="#dep-pipe-features" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-350">Here is n overview of the features implemented and integrated with other tools.</p>
<ul class="simple">
<li><p class="linemarker linemarker-352">Graphical user interface provided</p></li>
<li><p class="linemarker linemarker-353">Initialization of a deployment from the UI</p></li>
<li><p class="linemarker linemarker-354">R-MARKET resources usage</p></li>
<li><p class="linemarker linemarker-355">Deployment of pipelines using containerized steps</p></li>
<li><p class="linemarker linemarker-356">Deployment based on the descriptors provided by ADA-PIPE</p></li>
<li><p class="linemarker linemarker-357">Reception of resource configurations from R-MARKET through the ADA-PIPE descriptor</p></li>
<li><p class="linemarker linemarker-358">Multi-cloud support</p></li>
<li><p class="linemarker linemarker-359">Security policies enforcement</p></li>
<li><p class="linemarker linemarker-360">Scale cluster</p></li>
<li><p class="linemarker linemarker-361">Pre-deployment configuration</p></li>
<li><p class="linemarker linemarker-362">Adaptation based on pipeline chunks from ADA-PIPE</p></li>
<li><p class="linemarker linemarker-363">Vulnerabilities scanning</p></li>
</ul>
</section>
<section id="id13">
<h3>Integration with the Toolbox Components<a class="headerlink" href="#id13" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-368">To perform deployment in DEP-PIPE, DEF-PIPE provides a list of pipelines per user. Once the user selects the pipeline, ADA-PIPE provides DEP-PIPE with the pipeline chunk to deploy, and a description of the resources allocated to it.
On the image below can be seen how ADA-PIPE integrates with DEP-PIPE with the API.</p>
<img alt="../_images/apiDeploymentADAandDEP.png" src="../_images/apiDeploymentADAandDEP.png" />
<p class="linemarker linemarker-373">The JSON for this call is provided below, and based on the Json schema of the <a class="reference external" href="https://github.com/DataCloud-project/DEP-PIPE-Pipeline-Deployment-Controller/blob/main/samples/sample-file.json">DEP-PIPE</a> descriptor</p>
<div class="highlight-JSON notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w"> </span><span class="nt">&quot;pipelineName&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Heathcare_pipeline&quot;</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;pipelineType&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;simulation|production&quot;</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;stepName&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;readMedicValues&quot;</span><span class="p">,</span>
<span class="w"> </span><span class="nt">&quot;terminationCheck&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="nt">&quot;httpURL&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;localhost/api/v1/success&quot;</span>
<span class="w"> </span><span class="p">},</span>
<span class="w"> </span><span class="nt">&quot;time&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="nt">&quot;EST&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;EFT&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">2.92</span>
<span class="w"> </span><span class="p">},</span>
<span class="w"> </span><span class="nt">&quot;jobListjob&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">     </span><span class="nt">&quot;order&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;readMedicValues&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;resource&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;datacloud_worker_1_wp1&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;provider&quot;</span><span class="p">:</span><span class="s2">&quot;R-MARKET&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;architecture&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;amd64&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;elasticityControllerMode&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;HORIZONTAL&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;dockerImage&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;xx/xxx:1.0&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;dockerCredentialsUsing&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;false&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;dockerUsername&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;dockerPassword&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;dockerCustomRegistry&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;true&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;dockerRegistry&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;requirement&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;vCPUs&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;ram&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;storage&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span>
<span class="w">     </span><span class="p">},</span>
<span class="w">     </span><span class="nt">&quot;healthCheck&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;httpURL&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;localhost/api/v1/healthCheck&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ps -e | grep java&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;interval&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">20</span>
<span class="w">     </span><span class="p">},</span>
<span class="w">     </span><span class="nt">&quot;terminationCheck&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;httpURL&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;localhost/api/v1/success&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;args&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ps -e | grep java&quot;</span>
<span class="w">     </span><span class="p">},</span>
<span class="w">     </span><span class="nt">&quot;command&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--&gt;Container Excecution&lt;--&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;environmentalVariables&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">         </span><span class="nt">&quot;key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span>
<span class="w">     </span><span class="p">}],</span>
<span class="w">     </span><span class="nt">&quot;numWorkers&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;exposedInterfaces&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">         </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;sparkMasterRest&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;port&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;8000&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;interfaceType&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Core&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;transmissionProtocol&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;TCP&quot;</span>
<span class="w">     </span><span class="p">}],</span>
<span class="w">     </span><span class="nt">&quot;requiredInterfaces&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">         </span><span class="nt">&quot;friendlyNamename&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;sparkMasterRest&quot;</span>
<span class="w">     </span><span class="p">}],</span>
<span class="w">     </span><span class="nt">&quot;plugin&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">         </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span>
<span class="w">     </span><span class="p">}],</span>
<span class="w">     </span><span class="nt">&quot;devices&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">         </span><span class="nt">&quot;key&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/dev/gpu0&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;value&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;/dev/gpu0&quot;</span>
<span class="w">     </span><span class="p">}],</span>
<span class="w">     </span><span class="nt">&quot;volumes&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;!!!pending!!!&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;labels&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">         </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span>
<span class="w">     </span><span class="p">}],</span>
<span class="w">     </span><span class="nt">&quot;hostname&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;--&gt;Advanced Options&lt;--&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;capabilityDrops&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">         </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span>
<span class="w">     </span><span class="p">}],</span>
<span class="w">     </span><span class="nt">&quot;capabilityAdds&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span>
<span class="w">         </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span>
<span class="w">     </span><span class="p">}],</span>
<span class="w">     </span><span class="nt">&quot;ulimitMemlockSoft&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;ulimitMemlockHard&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;networkModeHost&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;true&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;privilege&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;false&quot;</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;publicComponet&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span>
<span class="w">     </span><span class="nt">&quot;dockerExecutionUser&quot;</span><span class="p">:</span><span class="s2">&quot;&quot;</span>
<span class="w"> </span><span class="p">}]</span>
</pre></div>
</div>
<p class="linemarker linemarker-455">}</p>
</section>
<section id="id14">
<h3>Deployment, Code and Documentation Availability<a class="headerlink" href="#id14" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-460">DEP-PIPE is currently split in four different repositories that are available in GitHub, and also uses MAESTRO that is hosted in UBITECH’s private registry.</p>
<img alt="../_images/repoDEPpipe.png" src="../_images/repoDEPpipe.png" />
<p class="linemarker linemarker-465">The main repository contains instructions for the DEP-PIPE setup (including MAESTRO tool setup instructions) in the main repository.</p>
<p class="linemarker linemarker-467">The DataCloud Deployment controller provides the main functionalities that allow the deployment of DataCloud pipelines, by transforming the ADA-PIPE provided JSON and create the needed steps (creation of dedicated containerized services) for the deployment through DataCloud. DataCloud Security Controller provides the backend services for the security functionalities (vulnerabilities scanning, access control). Finally, the monitoring service (that consists of a Monitoring collector, Prometheus and a service providing REST API) and data-drift libraries are provide in the dedicated repo.
DEP-PIPE has been deployed online and is accessible through the <a class="reference external" href="https://datacloud-dep.euprojects.net">https://datacloud-dep.euprojects.net</a> domain and is also part of the toolbox demo page (<a class="reference external" href="https://datacloud-toolbox.euprojects.net/#/deploy">https://datacloud-toolbox.euprojects.net/#/deploy</a>).</p>
</section>
</section>
<section id="runtime-dashboard-and-common-datacloud-ui">
<h2>Runtime Dashboard and Common DataCLoud UI<a class="headerlink" href="#runtime-dashboard-and-common-datacloud-ui" title="Link to this heading"></a></h2>
<p class="linemarker linemarker-475">In addition of providing standalone services, we also decided that it would be beneficial for the overall user experience of DataCloud to provide a common view that aggregates the functionalities of all tools. Initially, this led to the design of mock-ups, allowing such basic functionality.
In the first release, we proceeded with the creation of a common page that delivered the first PoC of the Toolbox, and since then, it has been used for accessing the tools deployed for demonstration and testing purposes.
The UI of the toolbox is accessible at <a class="reference external" href="https://datacloud-toolbox.euprojects.net">https://datacloud-toolbox.euprojects.net</a>.</p>
<img alt="../_images/toolboxLanding.png" src="../_images/toolboxLanding.png" />
<img alt="../_images/toolAccessedDatacoudUI.png" src="../_images/toolAccessedDatacoudUI.png" />
<p class="linemarker linemarker-483">The code for this common UI is available at: <a class="reference external" href="https://github.com/DataCloud-project/ALL-PIPE">https://github.com/DataCloud-project/ALL-PIPE</a>.
For the final release, we tried to simplify further the user experience, by integrating the tools further as part of the Runtime Dashboard. For this purpose, the database and the Apache <a class="reference external" href="https://kafka.apache.org">Kafka</a> message bus of DEP-PIPE have been used for any asynchronous communication of the components needed. This is considered especially in a platform where some actions can take a long time to be executed (e.g., data uploading or data anonymization), and asynchronous calls allow us to provide the user with a smoother user experience.</p>
<section id="common-identity-management">
<h3>Common Identity Management<a class="headerlink" href="#common-identity-management" title="Link to this heading"></a></h3>
<p class="linemarker linemarker-489">DataCloud supports a workflow that targets multiple users, that can have different background, such as data scientist and DataOps or business experts. For this purpose, we understand the importance of role management as part of an integrated platform as the various toolbox services might be targeting different users within an organization. Therefore, in DataCloud we use an Identity and Access Management (IAM) that support multiple user roles and we offer the possibility to create organisations that include multiple users (as part of the runtime management).</p>
<p class="linemarker linemarker-491"><strong>Keycloak Integration</strong></p>
<p class="linemarker linemarker-493">For DataCloud, <a class="reference external" href="https://www.keycloak.org">Keycloak</a>, an open-source solution, that has been deployed for the scope of the project</p>
<img alt="../_images/keycloak2.png" src="../_images/keycloak2.png" />
<p class="linemarker linemarker-498">Through configured realms and clients, Keycloak can centralize the login process of various systems and components through the implementation of protocols such as OAuth2.01 and OpenID Connect (a.k.a. OIDC)2. DataCloud tools integrated Keycloak, which for the public DataCloud toolbox, is deployed at <a class="reference external" href="https://datacloud-auth.euprojects.net/">https://datacloud-auth.euprojects.net/</a>, and thus, the toolbox features secure communication among the tools and proper Identity Management.</p>
<p class="linemarker linemarker-500">Below is an overview of the single sign-on integration inf DataCloud, covering front and back-end services.</p>
<img alt="../_images/authArchitecture.png" src="../_images/authArchitecture.png" />
<p class="linemarker linemarker-504">A user loading the web interface of a tools is prompted to log in (unless they have a valid session already). Once the user enters their credentials, they receive a secure token that enables them to access the application. In addition, if a service requests an asset from another DataCloud service (e.g., results of a simulation run is requested by the adaptation engine), it needs to provide a valid user token for the specific asset as part of the API call. The specific way that the userID will be provided has been agreed (based om the Keycloak tokens), while each service is responsible for storing this userID locally so that can find the appropriate user resources (e.g., the defined pipelines, simulations or deployments).</p>
<img alt="../_images/userIDacrossDatacloud.png" src="../_images/userIDacrossDatacloud.png" />
<p class="linemarker linemarker-508">Finally, from the user perspective, a common login and registration page has been created for the new users, as depicted in the following screens:</p>
<p class="linemarker linemarker-510"><a class="reference internal" href="../_images/registration.png"><img alt="pic1" src="../_images/registration.png" style="width: 49%;" /></a> <a class="reference internal" href="../_images/save.png"><img alt="pic2" src="../_images/save.png" style="width: 49%;" /></a></p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="Welcome to the online documentation of DataCloud Project and DataCloud Toolbox!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="b_documentation.html" class="btn btn-neutral float-right" title="Integrated Toolbox Documentation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, DataCloud consortium.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>